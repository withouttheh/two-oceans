{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q_nBGqsrDfm",
        "outputId": "2e2385aa-df30-4e8a-e0a3-5b4a2ff39f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "# import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "class ResultsScraper:\n",
        "    \"\"\"\n",
        "    A class to scrape and process results from a web page.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_url):\n",
        "        \"\"\"\n",
        "        Initialize the ResultsScraper object.\n",
        "\n",
        "        Parameters:\n",
        "            base_url (str): The base URL of the web page to scrape.\n",
        "        \"\"\"\n",
        "        self.base_url = base_url\n",
        "        self.column_names = None\n",
        "\n",
        "    def fetch_html_data(self, url):\n",
        "        \"\"\"\n",
        "        Fetch HTML data from a URL.\n",
        "\n",
        "        Parameters:\n",
        "            url (str): The URL to fetch HTML data from.\n",
        "\n",
        "        Returns:\n",
        "            str: The HTML data fetched from the URL.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # response = requests.get(url)\n",
        "            # response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "            # return response.text\n",
        "            pass  # Placeholder for actual implementation\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching HTML from {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def parse_html(self, html_data):\n",
        "        \"\"\"\n",
        "        Parse HTML data using BeautifulSoup.\n",
        "\n",
        "        Parameters:\n",
        "            html_data (str): The HTML data to parse.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup object: Parsed BeautifulSoup object.\n",
        "        \"\"\"\n",
        "        if html_data:\n",
        "            return BeautifulSoup(html_data, \"html.parser\")\n",
        "        return None\n",
        "\n",
        "    def get_column_names(self, soup):\n",
        "        \"\"\"\n",
        "        Extract column names from the HTML data.\n",
        "\n",
        "        Parameters:\n",
        "            soup (BeautifulSoup object): Parsed BeautifulSoup object.\n",
        "\n",
        "        Returns:\n",
        "            list: List of column names.\n",
        "        \"\"\"\n",
        "        if not self.column_names:\n",
        "            self.column_names = []\n",
        "            header_tags = soup.find_all('tr')[1].find_all('a')[:12]\n",
        "            for header_tag in header_tags:\n",
        "                self.column_names.append(header_tag.text)\n",
        "            self.column_names[self.column_names.index('Race No')] = 'Name'\n",
        "            self.column_names.insert(self.column_names.index('Time') + 1, 'Medal')\n",
        "        return self.column_names\n",
        "\n",
        "    def extract_results(self, soup, page_number):\n",
        "        \"\"\"\n",
        "        Extract results from the HTML data.\n",
        "\n",
        "        Parameters:\n",
        "            soup (BeautifulSoup object): Parsed BeautifulSoup object.\n",
        "            page_number (int): The page number of the results.\n",
        "\n",
        "        Returns:\n",
        "            list: List of dictionaries containing result data.\n",
        "        \"\"\"\n",
        "        if not soup:\n",
        "            return []\n",
        "        all_results = []\n",
        "        result_rows = soup.find_all('tr')[2:-1]\n",
        "        for row in result_rows:\n",
        "            result_data = row.find_all('td')[2:21]\n",
        "            pos = result_data[0].text\n",
        "            name = result_data[4].find('a').text\n",
        "            time = result_data[6].text\n",
        "            medal = result_data[7].text\n",
        "            club = result_data[8].text\n",
        "            gender = result_data[9].find('a').text\n",
        "            category = result_data[11].find('a').text\n",
        "            start = result_data[13].text\n",
        "            km_5 = result_data[14].text\n",
        "            km_14 = result_data[15].text\n",
        "            km_28 = result_data[16].text\n",
        "            km_42 = result_data[17].text\n",
        "            km_50 = result_data[18].text\n",
        "\n",
        "            result = {\n",
        "                \"Page No\": page_number,\n",
        "                \"Pos\": pos,\n",
        "                \"Name\": name,\n",
        "                \"Time\": time,\n",
        "                \"Medal\": medal,\n",
        "                \"Club\": club,\n",
        "                \"Gender\": gender,\n",
        "                \"Category\": category,\n",
        "                \"Start\": start,\n",
        "                \"5km\": km_5,\n",
        "                \"14km\": km_14,\n",
        "                \"28km\": km_28,\n",
        "                \"42.2km\": km_42,\n",
        "                \"50km\": km_50\n",
        "            }\n",
        "            all_results.append(result)\n",
        "        return all_results\n"
      ],
      "metadata": {
        "id": "UYpMJ71JLFKh"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base URL for scraping results\n",
        "base_url = \"https://results.finishtime.co.za/results.aspx?CId=35&RId=30353&EId=1&dt=0&PageNo=\"\n",
        "\n",
        "# Initialize an empty list to store all the scraped results\n",
        "all_results = []\n"
      ],
      "metadata": {
        "id": "CvVAgcN9e3jL"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch column names from the first page\n",
        "first_page_url = base_url + \"1\"\n",
        "first_page_scraper = ResultsScraper(base_url)\n",
        "first_page_html_data = first_page_scraper.fetch_html_data(first_page_url)\n",
        "first_page_soup = first_page_scraper.parse_html(first_page_html_data)\n",
        "column_names = first_page_scraper.get_column_names(first_page_soup)"
      ],
      "metadata": {
        "id": "1LlULxZTiAUX"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape and process subsequent pages\n",
        "for page_number in range(1, 246):\n",
        "    url = base_url + str(page_number)\n",
        "    scraper = ResultsScraper(base_url)\n",
        "    html_data = scraper.fetch_html_data(url)\n",
        "    soup = scraper.parse_html(html_data)\n",
        "    results = scraper.extract_results(soup, page_number)\n",
        "    all_results.extend(results)\n"
      ],
      "metadata": {
        "id": "QFZS5eFOysDU"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAAovVn-5gGx",
        "outputId": "ee3f8ae4-d511-49e9-bcf0-6a3f728d1600"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Page No': 1,\n",
              " 'Pos': '1',\n",
              " 'Name': 'ONALENNA KHONKHOBE',\n",
              " 'Time': '03:09:30',\n",
              " 'Medal': 'Gold',\n",
              " 'Club': 'NEDBANK DEVELOPMENT CLUB CENTRAL NORTH WEST',\n",
              " 'Gender': 'Male',\n",
              " 'Category': 'Senior',\n",
              " 'Start': '05:15:04',\n",
              " '5km': '00:17:38',\n",
              " '14km': '00:48:12',\n",
              " '28km': '01:36:39',\n",
              " '42.2km': '02:25:21',\n",
              " '50km': '02:52:00'}"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list of dictionaries to DataFrame\n",
        "df = pd.DataFrame(all_results)"
      ],
      "metadata": {
        "id": "2vk8toGxyxu0"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write DataFrame to CSV\n",
        "df.to_csv('56km_results.csv', index=False)"
      ],
      "metadata": {
        "id": "LQp1_rTI0ydk"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gjIRcTa3MMZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}